% Notes and exercises on Finite Dimensional Vector Spaces
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier}

% Sets
\newcommand{\F}{\mathbb{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\P}{\mathcal{P}}

% Relations
\newcommand{\iso}{\cong}

% Operations
\newcommand{\union}{\cup}
\newcommand{\sect}{\cap}
\newcommand{\dsum}{\oplus}
\newcommand{\bigunion}{\bigcup}
\newcommand{\bigsect}{\bigcap}

\DeclareMathOperator{\ran}{ran}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\spn}{span}

\newcommand{\restrict}[2]{{#1}|_{#2}}

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

\newtheoremstyle{direction}{0.5em}{0.5em}{}{}{}{}{0.5em}{}
\theoremstyle{direction}
\newtheorem*{fwd}{\(\implies\)}
\newtheorem*{bwd}{\(\impliedby\)}

% Meta
\title{\textit{Finite Dimensional Vector Spaces}\\Notes and Exercises}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle
% Chapter I
\section*{Chapter~I}
% Section 7
\subsection*{\S~7}
\begin{exer}[5]\
\begin{enumerate}
\item[(a)] Two vectors \(\vec{x}=(x_1,x_2)\) and \(\vec{y}=(y_1,y_2)\) in~\(\C^2\) are linearly dependent if and only if \(x_1y_2=x_2y_1\).
\item[(b)] Two vectors \(\vec{x}=(x_1,x_2,x_3)\) and \(\vec{y}=(y_1,y_2,y_3)\) in~\(\C^3\) are linearly dependent if and only if \(x_1y_2=x_2y_1\), \(x_1y_3=x_3y_1\), and \(x_2y_3=x_3y_2\).
\item[(c)] There is no set of three linearly independent vectors in~\(\C^2\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)]
\begin{fwd}
Since \(\vec{x}\)~and~\(\vec{y}\) are linearly dependent, there exist scalars \(\alpha,\beta\in\C\) not both zero such that \(\alpha\vec{x}+\beta\vec{y}=0\). If \(\alpha=0\), then \(\beta\ne0\), in which case we must have \(\vec{y}=0\) and the desired equality holds. Similarly if \(\beta=0\). Therefore we may assume \(\alpha\ne0\) and \(\beta\ne0\). We have
\begin{align*}
\alpha x_1&=-\beta y_1\\
\alpha x_2&=-\beta y_2
\end{align*}
Cross multiplying, we have
\[\alpha\beta x_1y_2=\alpha\beta x_2y_1\]
Since \(\alpha\beta\ne0\), the desired equality follows.
\end{fwd}
\begin{bwd}
We consider cases of~\(\vec{x}\):

If \(x_1\ne0\) and \(x_2\ne0\), let \(\alpha=y_1/x_1=y_2/x_2\). Then \(\alpha x_1=y_1\) and \(\alpha x_2=y_2\), so \(\alpha\vec{x}-\vec{y}=0\).

If \(x_1\ne0\) and \(x_2=0\), then \(y_2=0\), so \(\alpha\vec{x}-\vec{y}=0\) where \(\alpha=y_1/x_1\). Similarly if \(x_1=0\) and \(x_2\ne0\).

If \(x_1=0\) and \(x_2=0\), then linear independence is witnessed by \(\vec{x}=0\).
\end{bwd}
\item[(b)]
\begin{fwd}
As in~(a), except that now three cross multiplications are performed to yield the three equations.
\end{fwd}
\begin{bwd}
As in~(a), we consider cases of~\(\vec{x}\ne0\):

If \(x_1\ne0\), \(x_2\ne0\), and \(x_3\ne0\), let \(\alpha=y_1/x_1=y_2/x_2=y_3/x_3\). Then \(\alpha\vec{x}-\vec{y}=0\).

If \(x_i=0\), then \(y_i=0\) and the result follows from~(a) applied to the other coordinates.
\end{bwd}
Note geometrically this result is immediate from~(a) because two vectors in x-y-z space are linearly dependent if and only if their corresponding projections in each of the x-y, x-z, and y-z planes are linearly dependent.
\item[(c)]
We prove that any set of three vectors in~\(\C^2\) is linearly dependent. More specifically, if \(\vec{x},\vec{y},\vec{z}\in\C^2\) and \(\vec{x}\)~and~\(\vec{y}\) are linearly independent, then \(\vec{z}\)~is a linear combination of \(\vec{x}\)~and~\(\vec{y}\).\footnote{Equivalently, any set of two linearly independent vectors in~\(\C^2\) also spans~\(\C^2\) and hence is a basis for~\(\C^2\). See also Theorem~8.2.}

Indeed, suppose \(\vec{x}=(x_1,x_2)\) and \(\vec{y}=(y_1,y_2)\) are linearly independent. By part~(a), \(\delta=x_1y_2-x_2y_1\ne0\). Set
\[
\alpha=\frac{y_2z_1-y_1z_2}{\delta}
\qquad
\beta=\frac{x_1z_2-x_2z_1}{\delta}
\]
It is immediate that \(\alpha\vec{x}+\beta\vec{y}=\vec{z}\), as desired.

Note this result is also immediate from the fact that \(\dim\C^2=2<3\) (see Theorem~8.2).\qedhere
\end{enumerate}
\end{proof}

\begin{exer}[9]
There are 28~basis sets for~\(\C^3\) consisting of binary vectors (vectors each of whose coordinates is 0~or~1).
\end{exer}
\begin{proof}
We count the number of ways to construct a basis set.

There are \(2^3=8\) binary vectors. To construct a basis \emph{sequence}, we choose three linearly independent vectors from this set (Theorem~8.2). We see that there are 7~possible choices for the first vector, namely each of the nonzero binary vectors. For each of these choices, there are 6~possible choices for the second vector, namely each of the remaining nonzero binary vectors. Finally, for each of these \(7\cdot6=42\) choices, there are 4~possible choices for the third vector, namely each of the remaining binary vectors not in the span of the first two. This yields \(7\cdot6\cdot4=168\) possible basis sequences.

Since there are \(3\cdot2\cdot1=6\) sequences for each \emph{set} of three vectors, there are \(7\cdot4=28\) basis sets.
\end{proof}

% Section 9
\subsection*{\S~9}
\begin{exer}[2]
\(\R\)~is not finite dimensional over~\(\Q\).
\end{exer}
\begin{proof}
If it is, then \(\R\iso\Q^n\) for some~\(n\) (Theorem~1). But then
\[2^{\aleph_0}=\card\R=\card\Q^n=(\card\Q)^n=\aleph_0^n=\aleph_0\]
---a contradiction since \(2^{\aleph_0}>\aleph_0\).
\end{proof}

\begin{exer}[4]
Two rational vector spaces with the same cardinality need not be isomorphic.
\end{exer}
\begin{proof}
Consider \(\Q\)~and~\(\Q^2\). We have
\[\card{\Q}=\aleph_0=\aleph_0^2=(\card\Q)^2=\card\Q^2\]
However, \(\dim\Q=1<2=\dim\Q^2\), so \(\Q\not\iso\Q^2\).
\end{proof}

% Section 12
\subsection*{\S~12}
\begin{exer}[2]
If \(V\)~is a vector space and \(M\)~and~\(N\) are subspaces of~\(V\) satisfying \(V\subseteq M\union N\), then \(V=M\) or \(V=N\).
\end{exer}
\begin{proof}
Suppose \(V\ne M\) and \(V\ne N\). Then there exist vectors \(x\in V-M\) and \(y\in V-N\). Since \(V\subseteq M\union N\), we must have \(x\in N\) and \(y\in M\) and \(z=x+y\in M\union N\). But if \(z\in M\) then \(x=z-y\in M\), and if \(z\in N\) then \(y=z-x\in N\)---a contradiction in either case.
\end{proof}

\begin{exer}[6]
Let \(V\)~be a vector space and \(M\)~be a subspace of~\(V\).
\begin{enumerate}
\item[(a)] If \(M\)~is nontrivial (\(M\ne0\) and \(M\ne V\)), then \(M\)~does not have a unique complement.
\item[(b)] If \(V\)~is \(n\)-dimensional and \(M\)~is \(m\)-dimensional, then every complement of~\(M\) is (\(n-m\))-dimensional.
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] We claim that if \(x\in V-M\), then there exists a complement~\(N\) of~\(M\) with \(x\in N\). Indeed, if \(B\)~is any basis of~\(M\), then \(B\union\{x\}\) is linearly independent in~\(V\) and hence can be extended to a basis~\(B'\) of~\(V\). The subspace \(N=\spn(B'-B)\) is the desired complement.

By this result, if \(M\)~has unique complement~\(N\), then \(V-M\subseteq N\), so that \(V\subseteq M\union N\). But this implies \(V=M\) or \(V=N\) (Exercise~2). Since \(M\)~and~\(N\) are complements, \(V=N\) implies \(M=0\). Therefore, \(M\)~must be trivial.
\item[(b)]
If \(N\)~is a complement of~\(M\), let \(\{x_1,\ldots,x_m\}\) be a basis of~\(M\) and \(\{y_1,\ldots,y_k\}\) be a basis of~\(N\). Then \(\{x_1,\ldots,x_m,y_1,\ldots,y_k\}\) is a basis of~\(V\). Indeed, it spans~\(V\) since \(V=M+N\), and it is linearly independent since \(M\sect N=0\). Therefore \(n=m+k\), so \(\dim N=k=n-m\) as desired.\qedhere
\end{enumerate}
\end{proof}

\begin{exer}[7]
Let \(V\)~be a vector space and \(M\)~and~\(N\) be subspaces of~\(V\).
\begin{enumerate}
\item[(a)] If \(V\)~is 5-dimensional and \(M\)~and~\(N\) are 3-dimensional, then \(M\)~and~\(N\) are not disjoint.
\item[(b)] If \(M\)~and~\(N\) are finite dimensional, then
\[\dim M+\dim N=\dim(M+N)+\dim(M\sect N)\]
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] Since \(M+N\)~is a subspace of~\(V\), \(\dim(M+N)\le5\) (Theorem~1). By part~(b),
\[\dim(M\sect N)=\dim M+\dim N-\dim(M+N)\ge3+3-5=1>0\]
Therefore \(M\sect N\ne0\).
\item[(b)] Let \(m=\dim M\) and \(n=\dim N\). Since \(M\sect N\)~is a subspace of both \(M\)~and~\(N\), we know \(M\sect N\)~is finite dimensional and \(k=\dim(M\sect N)\le\min(m,n)\) (Theorem~1). Let \(\{x_1,\ldots,x_k\}\) be a basis of~\(M\sect N\). Extend it to a basis \(\{x_1,\ldots,x_k,y_1,\ldots,y_{m-k}\}\) of~\(M\) and to a basis \(\{x_1,\ldots,x_k,z_1,\ldots,z_{n-k}\}\) of~\(N\) (Theorem~2). Then
\[\{x_1,\ldots,x_k,y_1,\ldots,y_{m-k},z_1,\ldots,z_{n-k}\}\]
is a basis of~\(M+N\). Indeed, spanning and linear independence follow from the corresponding properties of the bases for \(M\)~and~\(N\). Therefore \(M+N\)~is finite dimensional and
\begin{align*}
\dim(M+N)&=k+(m-k)+(n-k)\\
	&=m+n-k\\
	&=\dim M+\dim N-\dim(M\sect N)\qedhere
\end{align*}
\end{enumerate}
\end{proof}
\begin{rmk}
This result is analogous to the inclusion-exclusion principle for sets:
\[\card(A\union B)=\card A+\card B-\card(A\sect B)\]
\end{rmk}

% Section 14
\subsection*{\S~14}
\begin{exer}[4]
Let \((\alpha_i)\in\C^{\infty}\). For \(x=\sum_{i=0}^n\xi_i t^i\in\P\), let \(y(x)=\sum_{i=0}^n\xi_i\alpha_i\). Then \(y\in\P'\), and every element in~\(\P'\) is of this form for suitable~\(\alpha_i\).
\end{exer}
\begin{proof}
Since the coefficients of~\(x\) are uniquely determined, \(y\)~is a well defined function from~\(\P\) to~\(\C\). If \(u=\sum_{i=0}^m\mu_i t^i\), \(v=\sum_{i=0}^n\nu_i t^i\), and \(\mu,\nu\in\C\), we may assume \(m=n\) (using coefficients of zero), and
\begin{align*}
y(\mu u+\nu v)&=y\bigl(\,\mu\sum\mu_i t^i+\nu\sum\nu_i t^i\,\bigr)\\
	&=y\bigl(\,\sum[\,\mu\mu_i+\nu\nu_i]t^i\,\bigr)\\
	&=\sum(\,\mu\mu_i+\nu\nu_i)\alpha_i\\
	&=\mu\sum\mu_i\alpha_i+\nu\sum\nu_i\alpha_i\\
	&=\mu y(u)+\nu y(v)
\end{align*}
Therefore \(y\)~is linear and hence \(y\in\P'\).

If \(z\in\P'\) is arbitrary, set \(\beta_i=[t^i,z]\). Then
\[\bigl[\,\sum\xi_i t^i,z\bigr]=\sum\xi_i[t^i,z]=\sum\xi_i\beta_i\]
so \(z\)~has the desired form for \((\beta_i)\in\C^{\infty}\).
\end{proof}

\begin{exer}[5]
If \(y\in V'\) and \(y\ne0\), and \(\alpha\in\F\) is an arbitrary scalar, then there exists \(x\in V\) with \([x,y]=\alpha\).
\end{exer}
\begin{proof}
Since \(y\ne0\), there exists \(x\in V\) with \(\beta=[x,y]\ne0\). Set \(\gamma=\alpha/\beta\). Then
\[[\gamma x,y]=\gamma[x,y]=\gamma\beta=\alpha\qedhere\]
\end{proof}

\begin{exer}[6]
If \(y,z\in V'\) and \([x,y]=0\) whenever \([x,z]=0\), then \(y=\alpha z\) for some \(\alpha\in\F\).
\end{exer}
\begin{proof}
If \(y=0\), take \(\alpha=0\). Otherwise, choose \(x_0\in V\) with \(\beta=[x_0,y]\ne0\). We must have \(\gamma=[x_0,z]\ne0\). Set \(\alpha=\beta/\gamma\). We claim \(y=\alpha z\).

Indeed, if there exists \(x\in V\) with \(\delta=[x,y]\ne[x,\alpha z]\), we must have \(\epsilon=[x,z]\ne0\). Set \(\zeta=\gamma/\epsilon\) and \(v=x_0-\zeta x\). Then
\[[v,z]=[x_0-\zeta x,z]=[x_0,z]-\zeta[x,z]=\gamma-\zeta\epsilon=\gamma-\gamma=0\]
but
\[[v,y]=[x_0-\zeta x,y]=[x_0,y]-\zeta[x,y]=\beta-\zeta\delta=\frac{\gamma(\alpha\epsilon-\delta)}{\epsilon}\ne0\]
---a contradiction.
\end{proof}

% Section 17
\subsection*{\S~17}
\begin{exer}[3]
If \(V\)~is a vector space and \(y\in V'\), define
\[K=\ker y=\{\,x\in V\mid [x,y]=0\,\}\]
Then \(K\)~is a subspace of~\(V\) and if \(n=\dim V\), then
\[\dim K=\begin{cases}
n&\text{if }y=0\\
n-1&\text{if }y\ne0
\end{cases}\]
\end{exer}
\begin{proof}
We have \(0\in K\) since \([0,y]=0\), and if \(u,v\in K\) and \(\alpha,\beta\in\F\), then
\[[\alpha u+\beta v,y]=\alpha[u,y]+\beta[v,y]=\alpha\cdot0+\beta\cdot0=0\]
so \(\alpha u+\beta v\in K\). Therefore \(K\)~is a subspace of~\(V\).

If \(n=\dim V\), let \(\{x_1,\ldots,x_k,x_{k+1},\ldots,x_n\}\) be a basis of~\(V\) where \(\{x_1,\ldots,x_k\}\) is a basis of~\(K\) (Theorem~12.2). Let \(U=\spn\{x_{k+1},\ldots,x_n\}\). Then
\[\dim V=n=k+(n-k)=\dim K+\dim U\]
We claim \(\restrict{y}{U}\)~is injective. Indeed, if \(u,v\in U\) and \([u,y]=[v,y]\), then \([u-v,y]=0\), so \(u-v\in K\). Write \(u=\sum_j\alpha_j x_{k+j}\) and \(v=\sum_j\beta_j x_{k+j}\). Then \(u-v=\sum_j(\alpha_j-\beta_j)x_{k+j}\). Now for the basis of~\(K\) there exist~\(\gamma_i\in\F\) such that
\[\sum_{j=1}^{n-k}(\alpha_j-\beta_j)x_{k+j}=\sum_{i=1}^k\gamma_i x_i\]
By linear independence of the basis for~\(V\), we must have \(\alpha_j-\beta_j=\gamma_i=0\) for all~\(i,j\). In particular, \(\alpha_j=\beta_j\) for all~\(j\), so \(u=v\), establishing injectivity.

We also claim \(\ran\restrict{y}{U}=\ran y\). Indeed, trivially \(\ran\restrict{y}{U}\subseteq\ran y\). Conversely, for any \(x=\sum_i\alpha_i x_i\in V\), we have
\begin{align*}
[x,y]&=[\alpha_1x_1+\cdots+\alpha_kx_k+\alpha_{k+1}x_{k+1}+\cdots+\alpha_n x_n,y]\\
	&=\alpha_1[x_1,y]+\cdots+\alpha_k[x_k,y]+\alpha_{k+1}[x_{k+1},y]+\cdots+\alpha_n[x_n,y]\\
	&=\alpha_1\cdot0+\cdots+\alpha_k\cdot0+\alpha_{k+1}[x_{k+1},y]+\cdots+\alpha_n[x_n,y]\\
	&=[\alpha_{k+1}x_{k+1}+\cdots+\alpha_n x_n,y]
\end{align*}
Since \(u=\sum_j\alpha_{k+j}x_{k+j}\in U\), this shows \(\ran y\subseteq\ran\restrict{y}{U}\). Hence \(\restrict{y}{U}:U\iso\ran y\).

Now if \(y=0\), then \(\ran y=0\) so \(\dim U=0\) and \(\dim K=n\). If \(y\ne0\), then \(\ran y=\F\) (Exercise~14.5), so \(\dim U=\dim\F=1\) and \(\dim K=n-1\).
\end{proof}
\begin{rmk}
This result is just a special case of rank nullity (Theorem~50.1), which asserts that \(\dim V=\dim\ker T+\dim\ran T\) for any linear transformation~\(T\) on~\(V\).
\end{rmk}

\begin{exer}[4]
Let \(y\in(\C^3)'\) defined by
\[(x_1,x_2,x_3)\mapsto x_1+x_2+x_3\]
Then \(B=\{(1,0,-1),(0,1,-1)\}\) is a basis of~\(\ker y\).
\end{exer}
\begin{proof}
Clearly, \(B\subseteq\ker y\) and \(B\)~is linearly independent. Since \(y\ne0\), \(\dim\ker y=3-1=2\) (Exercise~3). Therefore \(B\)~is a basis of~\(\ker y\) (Theorem~8.2).
\end{proof}

\begin{exer}[5]
If \(V\)~is an \(n\)-dimensional vector space and \(y_1,\ldots,y_m\) are linear functionals on~\(V\) where \(m<n\), then there exists a nonzero \(x\in V\) such that \([x,y_j]=0\) for all \(1\le j\le m\).
\end{exer}
\begin{proof}
We need to show that \(\bigsect\ker y_j\ne0\). First, we may assume without loss of generality that \(y_j\ne0\) for all \(1\le j\le m\), so \(\dim\ker y_j=n-1\) for all \(1\le j\le m\) (Exercise~3). We claim that
\[\dim\bigsect_{j=1}^m\ker y_j\ge n-m\]
The desired result then follows since \(n-m>0\).

We proceed by induction on~\(m\). The claim is true for \(m=1\) by the above. For \(m>1\), we have
\[\bigsect_{j=1}^m\ker y_j=\bigl(\bigsect_{j=1}^{m-1}\ker y_j\bigr)\sect\ker y_m\]
Therefore, by the inclusion-exclusion principle for dimension (Exercise~12.7) and the induction hypothesis, we have
\begin{align*}
\dim\bigsect_{j=1}^m\ker y_j&=\dim\Bigl[\bigl(\bigsect_{j=1}^{m-1}\ker y_j\bigr)\sect\ker y_m\Bigr]\\
	&=\dim\bigsect_{j=1}^{m-1}\ker y_j+\dim\ker y_m-\dim\Bigl[\bigsect_{j=1}^{m-1}\ker y_j+\ker y_m\Bigr]\\
	&\ge[n-(m-1)]+(n-1)-n\\
	&=n-m\qedhere
\end{align*}
\end{proof}
\begin{rmk}
This result implies that a homogeneous system of \(m\)~linear equations in \(n\)~variables always has a nontrivial solution when \(m<n\). Indeed, consider the system
\begin{equation*}
\left\lbrace
\begin{aligned}
\alpha_{11}x_1+\cdots+\alpha_{1n}x_n&=0\\
\phantom{\alpha_{i1}}\vdots\phantom{+\cdots+a_{in}}\vdots\phantom{x_{in}}&\phantom{=}\vdots\\
\alpha_{m1}x_1+\cdots+\alpha_{mn}x_n&=0
\end{aligned}
\right.\qquad(\alpha_{ij}\in\F)
\end{equation*}
For \(x=(x_1,\ldots,x_n)\in\F^n\), define~\(y_j\) by \([x,y_j]=\sum_i\alpha_{ij}x_i\) for \(1\le j\le m\). Clearly this system has a nontrivial solution if and only if there exists a nonzero \(x\in\F^n\) such that \([x,y_j]=0\) for all \(1\le j\le m\), which is true by this result.
\end{rmk}

\begin{exer}[7]
If \(V\)~is an \(n\)-dimensional vector space and \(0\le m\le n\), then the number of \(m\)-dimensional subspaces of~\(V\) is equal to the number of \((n-m)\)-dimensional subspaces.
\end{exer}
\begin{proof}
Fix a basis of~\(V\) and assume that \(V=V'=V''\) (Theorems 15.2~and~16.1). Now the mapping \(M\mapsto M^0\) sends each \(m\)-dimensional subspace to an \((n-m)\)-dimensional subspace (Theorem~1). Moreover, this mapping is its own inverse (Theorem~2), hence it is bijective and witnesses cardinal equality.
\end{proof}

% Chapter II
\section*{Chapter~II}
% Section 49
\subsection*{\S~49}
\begin{exer}[4]
Let \(V\)~be a vector space and \(E\)~and~\(F\) be projections on~\(V\).
\begin{enumerate}[itemsep=0pt]
\item[(a)] \(\ran(E)=\ran(F)\) if and only if \(EF=F\) and \(FE=E\).
\item[(b)] \(\ker(E)=\ker(F)\) if and only if \(EF=E\) and \(FE=F\).
\end{enumerate}
\end{exer}
\begin{proof}
Recall for a projection~\(P\) on~\(V\), \(V=\ran(P)\dsum\ker(P)\) and (Theorem~41.2)
\[\ran(P)=\{\,x\in V\mid Px=x\,\}\qquad\ker(P)=\{\,x\in V\mid Px=0\,\}\]
\begin{enumerate}[itemsep=0pt]
\item[(a)]
\begin{fwd}
If \(x\in V\), then \(Ex\in\ran(E)\subseteq\ran(F)\), so \(FEx=F(Ex)=Ex\). Therefore \(FE=E\). Similarly \(EF=F\).
\end{fwd}
\begin{bwd}
If \(x\in\ran(E)\), then \(x=Eu\) for some \(u\in V\), so
\[Fx=F(Eu)=FEu=Eu=x\]
and hence \(x\in\ran(F)\). Therefore \(\ran(E)\subseteq\ran(F)\). Similarly \(\ran(F)\subseteq\ran(E)\) and hence \(\ran(E)=\ran(F)\).
\end{bwd}
\item[(b)]
\begin{fwd}
Since \(V=\ran(E)\dsum\ker(E)\), if \(x\in V\) there exist \(u\in\ran(E)\) and \(v\in\ker(E)\) with \(x=u+v\). Now
\begin{align*}
FEx&=FE(u+v)&&\\
	&=FEu+FEv&&\\
	&=Fu+F0&&\text{since \(u\in\ran(E)\) and \(v\in\ker(E)\)}\\
	&=Fu+Fv&&\text{since \(\ker(E)\subseteq\ker(F)\)}\\
	&=F(u+v)\\
	&=Fx
\end{align*}
Therefore \(FE=F\). Similarly \(EF=E\).
\end{fwd}
\begin{bwd}
If \(x\in\ker(E)\), then
\[Fx=FEx=F(Ex)=F0=0\]
so \(x\in\ker(F)\). Therefore \(\ker(E)\subseteq\ker(F)\). Similarly \(\ker(F)\subseteq\ker(E)\) and hence \(\ker(E)=\ker(F)\).\qedhere
\end{bwd}
\end{enumerate}
\end{proof}
\begin{rmk}
By (a)~and~(b), \(E=F\) if and only if \(\ran(E)=\ran(F)\) and \(\ker(E)=\ker(F)\). In other words, projections are characterized by their ranges and null spaces.
\end{rmk}

\begin{exer}[5]
If \(E_1,\ldots,E_k\) are projections on~\(V\) with the same range and \(\alpha_1,\ldots,\alpha_k\) are scalars such that \(\sum_i\alpha_i=1\), then \(E=\sum_i\alpha_i E_i\) is a projection.
\end{exer}
\begin{proof}
By Exercise~4(a), we have
\begin{align*}
E^2&=\bigl(\,\sum_i\alpha_iE_i\bigr)^2&&\\
	&=\sum_i\sum_j\alpha_i\alpha_j E_iE_j&&\\
	&=\sum_i\sum_j\alpha_i\alpha_j E_j&&\text{since \(\ran(E_i)=\ran(E_j)\)}\\
	&=\bigl(\,\sum_i\alpha_i\bigr)\bigl(\,\sum_j\alpha_jE_j\bigr)&&\\
	&=1\cdot E&&\text{since \(\textstyle\sum_i\alpha_i=1\)}\\
	&=E
\end{align*}
Therefore \(E\)~is idempotent, and hence a projection (Theorem~41.1).
\end{proof}

% References
\begin{thebibliography}{0}
\bibitem{halmos87} Halmos, P. \textit{Finite Dimensional Vector Spaces.} Springer, 1987.
\end{thebibliography}
\end{document}