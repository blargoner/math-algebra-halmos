% Notes and exercises on Finite Dimensional Vector Spaces
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier}

% Sets
\newcommand{\F}{\mathbb{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\P}{\mathcal{P}}

% Relations
\newcommand{\iso}{\cong}

% Operations
\newcommand{\union}{\cup}
\newcommand{\sect}{\cap}
\newcommand{\dsum}{\oplus}
\newcommand{\tprod}{\otimes}
\newcommand{\bigunion}{\bigcup}
\newcommand{\bigsect}{\bigcap}

\DeclareMathOperator{\ran}{ran}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\spn}{span}

\newcommand{\pair}[2]{\langle{#1},{#2}\rangle}
\newcommand{\restrict}[2]{{#1}|_{#2}}

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

\newtheoremstyle{direction}{0.5em}{0.5em}{}{}{}{}{0.5em}{}
\theoremstyle{direction}
\newtheorem*{fwd}{\(\implies\)}
\newtheorem*{bwd}{\(\impliedby\)}

% Meta
\title{\textit{Finite Dimensional Vector Spaces}\\Notes and Exercises}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle
% Chapter I
\section*{Chapter~I}
% Section 7
\subsection*{\S~7}
\begin{exer}[5]\
\begin{enumerate}
\item[(a)] Two vectors \(\vec{x}=(x_1,x_2)\) and \(\vec{y}=(y_1,y_2)\) in~\(\C^2\) are linearly dependent if and only if \(x_1y_2=x_2y_1\).
\item[(b)] Two vectors \(\vec{x}=(x_1,x_2,x_3)\) and \(\vec{y}=(y_1,y_2,y_3)\) in~\(\C^3\) are linearly dependent if and only if \(x_1y_2=x_2y_1\), \(x_1y_3=x_3y_1\), and \(x_2y_3=x_3y_2\).
\item[(c)] There is no set of three linearly independent vectors in~\(\C^2\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)]
\begin{fwd}
Since \(\vec{x}\)~and~\(\vec{y}\) are linearly dependent, there exist scalars \(\alpha,\beta\in\C\) not both zero such that \(\alpha\vec{x}+\beta\vec{y}=0\). If \(\alpha=0\), then \(\beta\ne0\), in which case we must have \(\vec{y}=0\) and the desired equality holds. Similarly if \(\beta=0\). Therefore we may assume \(\alpha\ne0\) and \(\beta\ne0\). We have
\begin{align*}
\alpha x_1&=-\beta y_1\\
\alpha x_2&=-\beta y_2
\end{align*}
Cross multiplying, we have
\[\alpha\beta x_1y_2=\alpha\beta x_2y_1\]
Since \(\alpha\beta\ne0\), the desired equality follows.
\end{fwd}
\begin{bwd}
We consider cases of~\(\vec{x}\):

If \(x_1\ne0\) and \(x_2\ne0\), let \(\alpha=y_1/x_1=y_2/x_2\). Then \(\alpha x_1=y_1\) and \(\alpha x_2=y_2\), so \(\alpha\vec{x}-\vec{y}=0\).

If \(x_1\ne0\) and \(x_2=0\), then \(y_2=0\), so \(\alpha\vec{x}-\vec{y}=0\) where \(\alpha=y_1/x_1\). Similarly if \(x_1=0\) and \(x_2\ne0\).

If \(x_1=0\) and \(x_2=0\), then linear independence is witnessed by \(\vec{x}=0\).
\end{bwd}
\item[(b)]
\begin{fwd}
As in~(a), except that now three cross multiplications are performed to yield the three equations.
\end{fwd}
\begin{bwd}
As in~(a), we consider cases of~\(\vec{x}\ne0\):

If \(x_1\ne0\), \(x_2\ne0\), and \(x_3\ne0\), let \(\alpha=y_1/x_1=y_2/x_2=y_3/x_3\). Then \(\alpha\vec{x}-\vec{y}=0\).

If \(x_i=0\), then \(y_i=0\) and the result follows from~(a) applied to the other coordinates.
\end{bwd}
Note geometrically this result is immediate from~(a) because two vectors in x-y-z space are linearly dependent if and only if their corresponding projections in each of the x-y, x-z, and y-z planes are linearly dependent.
\item[(c)]
We prove that any set of three vectors in~\(\C^2\) is linearly dependent. More specifically, if \(\vec{x},\vec{y},\vec{z}\in\C^2\) and \(\vec{x}\)~and~\(\vec{y}\) are linearly independent, then \(\vec{z}\)~is a linear combination of \(\vec{x}\)~and~\(\vec{y}\).\footnote{Equivalently, any set of two linearly independent vectors in~\(\C^2\) also spans~\(\C^2\) and hence is a basis for~\(\C^2\). See also Theorem~8.2.}

Indeed, suppose \(\vec{x}=(x_1,x_2)\) and \(\vec{y}=(y_1,y_2)\) are linearly independent. By part~(a), \(\delta=x_1y_2-x_2y_1\ne0\). Set
\[
\alpha=\frac{y_2z_1-y_1z_2}{\delta}
\qquad
\beta=\frac{x_1z_2-x_2z_1}{\delta}
\]
It is immediate that \(\alpha\vec{x}+\beta\vec{y}=\vec{z}\), as desired.

Note this result is also immediate from the fact that \(\dim\C^2=2<3\) (see Theorem~8.2).\qedhere
\end{enumerate}
\end{proof}

\begin{exer}[9]
There are 28~basis sets for~\(\C^3\) consisting of binary vectors (vectors each of whose coordinates is 0~or~1).
\end{exer}
\begin{proof}
We count the number of ways to construct a basis set.

There are \(2^3=8\) binary vectors. To construct a basis \emph{sequence}, we choose three linearly independent vectors from this set (Theorem~8.2). We see that there are 7~possible choices for the first vector, namely each of the nonzero binary vectors. For each of these choices, there are 6~possible choices for the second vector, namely each of the remaining nonzero binary vectors. Finally, for each of these \(7\cdot6=42\) choices, there are 4~possible choices for the third vector, namely each of the remaining binary vectors not in the span of the first two. This yields \(7\cdot6\cdot4=168\) possible basis sequences.

Since there are \(3\cdot2\cdot1=6\) sequences for each \emph{set} of three vectors, there are \(7\cdot4=28\) basis sets.
\end{proof}

% Section 9
\subsection*{\S~9}
\begin{exer}[2]
\(\R\)~is not finite dimensional over~\(\Q\).
\end{exer}
\begin{proof}
If it is, then \(\R\iso\Q^n\) for some~\(n\) (Theorem~1). But then
\[2^{\aleph_0}=\card\R=\card\Q^n=(\card\Q)^n=\aleph_0^n=\aleph_0\]
---a contradiction since \(2^{\aleph_0}>\aleph_0\).
\end{proof}

\begin{exer}[4]
Two rational vector spaces with the same cardinality need not be isomorphic.
\end{exer}
\begin{proof}
Consider \(\Q\)~and~\(\Q^2\). We have
\[\card{\Q}=\aleph_0=\aleph_0^2=(\card\Q)^2=\card\Q^2\]
However, \(\dim\Q=1<2=\dim\Q^2\), so \(\Q\not\iso\Q^2\).
\end{proof}

% Section 12
\subsection*{\S~12}
\begin{exer}[2]
If \(V\)~is a vector space and \(M\)~and~\(N\) are subspaces of~\(V\) satisfying \(V\subseteq M\union N\), then \(V=M\) or \(V=N\).
\end{exer}
\begin{proof}
Suppose \(V\ne M\) and \(V\ne N\). Then there exist vectors \(x\in V-M\) and \(y\in V-N\). Since \(V\subseteq M\union N\), we must have \(x\in N\) and \(y\in M\) and \(z=x+y\in M\union N\). But if \(z\in M\) then \(x=z-y\in M\), and if \(z\in N\) then \(y=z-x\in N\)---a contradiction in either case.
\end{proof}

\begin{exer}[6]
Let \(V\)~be a vector space and \(M\)~be a subspace of~\(V\).
\begin{enumerate}
\item[(a)] If \(M\)~is nontrivial (\(M\ne0\) and \(M\ne V\)), then \(M\)~does not have a unique complement.
\item[(b)] If \(V\)~is \(n\)-dimensional and \(M\)~is \(m\)-dimensional, then every complement of~\(M\) is (\(n-m\))-dimensional.
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] We claim that if \(x\in V-M\), then there exists a complement~\(N\) of~\(M\) with \(x\in N\). Indeed, if \(B\)~is any basis of~\(M\), then \(B\union\{x\}\) is linearly independent in~\(V\) and hence can be extended to a basis~\(B'\) of~\(V\). The subspace \(N=\spn(B'-B)\) is the desired complement.

By this result, if \(M\)~has unique complement~\(N\), then \(V-M\subseteq N\), so that \(V\subseteq M\union N\). But this implies \(V=M\) or \(V=N\) (Exercise~2). Since \(M\)~and~\(N\) are complements, \(V=N\) implies \(M=0\). Therefore, \(M\)~must be trivial.
\item[(b)]
If \(N\)~is a complement of~\(M\), let \(\{x_1,\ldots,x_m\}\) be a basis of~\(M\) and \(\{y_1,\ldots,y_k\}\) be a basis of~\(N\). Then \(\{x_1,\ldots,x_m,y_1,\ldots,y_k\}\) is a basis of~\(V\). Indeed, it spans~\(V\) since \(V=M+N\), and it is linearly independent since \(M\sect N=0\). Therefore \(n=m+k\), so \(\dim N=k=n-m\) as desired.\qedhere
\end{enumerate}
\end{proof}

\begin{exer}[7]
Let \(V\)~be a vector space and \(M\)~and~\(N\) be subspaces of~\(V\).
\begin{enumerate}
\item[(a)] If \(V\)~is 5-dimensional and \(M\)~and~\(N\) are 3-dimensional, then \(M\)~and~\(N\) are not disjoint.
\item[(b)] If \(M\)~and~\(N\) are finite dimensional, then
\[\dim M+\dim N=\dim(M+N)+\dim(M\sect N)\]
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] Since \(M+N\)~is a subspace of~\(V\), \(\dim(M+N)\le5\) (Theorem~1). By part~(b),
\[\dim(M\sect N)=\dim M+\dim N-\dim(M+N)\ge3+3-5=1>0\]
Therefore \(M\sect N\ne0\).
\item[(b)] Let \(m=\dim M\) and \(n=\dim N\). Since \(M\sect N\)~is a subspace of both \(M\)~and~\(N\), we know \(M\sect N\)~is finite dimensional and \(k=\dim(M\sect N)\le\min(m,n)\) (Theorem~1). Let \(\{x_1,\ldots,x_k\}\) be a basis of~\(M\sect N\). Extend it to a basis \(\{x_1,\ldots,x_k,y_1,\ldots,y_{m-k}\}\) of~\(M\) and to a basis \(\{x_1,\ldots,x_k,z_1,\ldots,z_{n-k}\}\) of~\(N\) (Theorem~2). Then
\[\{x_1,\ldots,x_k,y_1,\ldots,y_{m-k},z_1,\ldots,z_{n-k}\}\]
is a basis of~\(M+N\). Indeed, spanning and linear independence follow from the corresponding properties of the bases for \(M\)~and~\(N\). Therefore \(M+N\)~is finite dimensional and
\begin{align*}
\dim(M+N)&=k+(m-k)+(n-k)\\
	&=m+n-k\\
	&=\dim M+\dim N-\dim(M\sect N)\qedhere
\end{align*}
\end{enumerate}
\end{proof}
\begin{rmk}
This result is analogous to the inclusion-exclusion principle for sets:
\[\card(A\union B)=\card A+\card B-\card(A\sect B)\]
\end{rmk}

% Section 14
\subsection*{\S~14}
\begin{exer}[4]
Let \((\alpha_i)\in\C^{\infty}\). For \(x=\sum_{i=0}^n\xi_i t^i\in\P\), let \(y(x)=\sum_{i=0}^n\xi_i\alpha_i\). Then \(y\in\P'\), and every element in~\(\P'\) is of this form for suitable~\(\alpha_i\).
\end{exer}
\begin{proof}
Since the coefficients of~\(x\) are uniquely determined, \(y\)~is a well defined function from~\(\P\) to~\(\C\). If \(u=\sum_{i=0}^m\mu_i t^i\), \(v=\sum_{i=0}^n\nu_i t^i\), and \(\mu,\nu\in\C\), we may assume \(m=n\) (using coefficients of zero), and
\begin{align*}
y(\mu u+\nu v)&=y\bigl(\,\mu\sum_i\mu_i t^i+\nu\sum_i\nu_i t^i\,\bigr)\\
	&=y\bigl(\,\sum_i[\,\mu\mu_i+\nu\nu_i]t^i\,\bigr)\\
	&=\sum_i(\,\mu\mu_i+\nu\nu_i)\alpha_i\\
	&=\mu\sum_i\mu_i\alpha_i+\nu\sum_i\nu_i\alpha_i\\
	&=\mu y(u)+\nu y(v)
\end{align*}
Therefore \(y\)~is linear and hence \(y\in\P'\).

If \(z\in\P'\) is arbitrary, set \(\beta_i=[t^i,z]\). Then
\[\bigl[\,\sum_i\xi_i t^i,z\bigr]=\sum_i\xi_i[t^i,z]=\sum_i\xi_i\beta_i\]
so \(z\)~has the desired form for \((\beta_i)\in\C^{\infty}\).
\end{proof}

\begin{exer}[5]
If \(y\in V'\) and \(y\ne0\), and \(\alpha\in\F\) is an arbitrary scalar, then there exists \(x\in V\) with \([x,y]=\alpha\).
\end{exer}
\begin{proof}
Since \(y\ne0\), there exists \(x\in V\) with \(\beta=[x,y]\ne0\). Set \(\gamma=\alpha/\beta\). Then
\[[\gamma x,y]=\gamma[x,y]=\gamma\beta=\alpha\qedhere\]
\end{proof}

\begin{exer}[6]
If \(y,z\in V'\) and \([x,y]=0\) whenever \([x,z]=0\), then \(y=\alpha z\) for some \(\alpha\in\F\).
\end{exer}
\begin{proof}
If \(y=0\), take \(\alpha=0\). Otherwise, choose \(x_0\in V\) with \(\beta=[x_0,y]\ne0\). We must have \(\gamma=[x_0,z]\ne0\). Set \(\alpha=\beta/\gamma\). We claim \(y=\alpha z\).

Indeed, if there exists \(x\in V\) with \(\delta=[x,y]\ne[x,\alpha z]\), we must have \(\epsilon=[x,z]\ne0\). Set \(\zeta=\gamma/\epsilon\) and \(v=x_0-\zeta x\). Then
\[[v,z]=[x_0-\zeta x,z]=[x_0,z]-\zeta[x,z]=\gamma-\zeta\epsilon=\gamma-\gamma=0\]
but
\[[v,y]=[x_0-\zeta x,y]=[x_0,y]-\zeta[x,y]=\beta-\zeta\delta=\frac{\gamma(\alpha\epsilon-\delta)}{\epsilon}\ne0\]
---a contradiction.
\end{proof}

% Section 17
\subsection*{\S~17}
\begin{exer}[3]
If \(V\)~is a vector space and \(y\in V'\), define
\[K=\ker y=\{\,x\in V\mid [x,y]=0\,\}\]
Then \(K\)~is a subspace of~\(V\) and if \(n=\dim V\), then
\[\dim K=\begin{cases}
n&\text{if }y=0\\
n-1&\text{if }y\ne0
\end{cases}\]
\end{exer}
\begin{proof}
We have \(0\in K\) since \([0,y]=0\), and if \(u,v\in K\) and \(\alpha,\beta\in\F\), then
\[[\alpha u+\beta v,y]=\alpha[u,y]+\beta[v,y]=\alpha\cdot0+\beta\cdot0=0\]
so \(\alpha u+\beta v\in K\). Therefore \(K\)~is a subspace of~\(V\).

If \(n=\dim V\), let \(\{x_1,\ldots,x_k,x_{k+1},\ldots,x_n\}\) be a basis of~\(V\) where \(\{x_1,\ldots,x_k\}\) is a basis of~\(K\) (Theorem~12.2). Let \(U=\spn\{x_{k+1},\ldots,x_n\}\). Then
\[\dim V=n=k+(n-k)=\dim K+\dim U\]
We claim \(\restrict{y}{U}\)~is injective. Indeed, if \(u,v\in U\) and \([u,y]=[v,y]\), then \([u-v,y]=0\), so \(u-v\in K\). Write \(u=\sum_j\alpha_j x_{k+j}\) and \(v=\sum_j\beta_j x_{k+j}\). Then \(u-v=\sum_j(\alpha_j-\beta_j)x_{k+j}\). Now for the basis of~\(K\) there exist~\(\gamma_i\in\F\) such that
\[\sum_{j=1}^{n-k}(\alpha_j-\beta_j)x_{k+j}=\sum_{i=1}^k\gamma_i x_i\]
By linear independence of the basis for~\(V\), we must have \(\alpha_j-\beta_j=\gamma_i=0\) for all~\(i,j\). In particular, \(\alpha_j=\beta_j\) for all~\(j\), so \(u=v\), establishing injectivity.

We also claim \(\ran\restrict{y}{U}=\ran y\). Indeed, trivially \(\ran\restrict{y}{U}\subseteq\ran y\). Conversely, for any \(x=\sum_i\alpha_i x_i\in V\), we have
\begin{align*}
[x,y]&=[\alpha_1x_1+\cdots+\alpha_kx_k+\alpha_{k+1}x_{k+1}+\cdots+\alpha_n x_n,y]\\
	&=\alpha_1[x_1,y]+\cdots+\alpha_k[x_k,y]+\alpha_{k+1}[x_{k+1},y]+\cdots+\alpha_n[x_n,y]\\
	&=\alpha_1\cdot0+\cdots+\alpha_k\cdot0+\alpha_{k+1}[x_{k+1},y]+\cdots+\alpha_n[x_n,y]\\
	&=[\alpha_{k+1}x_{k+1}+\cdots+\alpha_n x_n,y]
\end{align*}
Since \(u=\sum_j\alpha_{k+j}x_{k+j}\in U\), this shows \(\ran y\subseteq\ran\restrict{y}{U}\). Hence \(\restrict{y}{U}:U\iso\ran y\).

Now if \(y=0\), then \(\ran y=0\) so \(\dim U=0\) and \(\dim K=n\). If \(y\ne0\), then \(\ran y=\F\) (Exercise~14.5), so \(\dim U=\dim\F=1\) and \(\dim K=n-1\).
\end{proof}
\begin{rmk}
This result is just a special case of rank nullity (Theorem~50.1), which asserts that \(\dim V=\dim\ker T+\dim\ran T\) for any linear transformation~\(T\) on~\(V\).
\end{rmk}

\begin{exer}[4]
Let \(y\in(\C^3)'\) defined by
\[(x_1,x_2,x_3)\mapsto x_1+x_2+x_3\]
Then \(B=\{(1,0,-1),(0,1,-1)\}\) is a basis of~\(\ker y\).
\end{exer}
\begin{proof}
Clearly, \(B\subseteq\ker y\) and \(B\)~is linearly independent. Since \(y\ne0\), \(\dim\ker y=3-1=2\) (Exercise~3). Therefore \(B\)~is a basis of~\(\ker y\) (Theorem~8.2).
\end{proof}

\begin{exer}[5]
If \(V\)~is an \(n\)-dimensional vector space and \(y_1,\ldots,y_m\) are linear functionals on~\(V\) where \(m<n\), then there exists a nonzero \(x\in V\) such that \([x,y_j]=0\) for all \(1\le j\le m\).
\end{exer}
\begin{proof}
We need to show that \(\bigsect\ker y_j\ne0\). First, we may assume without loss of generality that \(y_j\ne0\) for all \(1\le j\le m\), so \(\dim\ker y_j=n-1\) for all \(1\le j\le m\) (Exercise~3). We claim that
\[\dim\bigsect_{j=1}^m\ker y_j\ge n-m\]
The desired result then follows since \(n-m>0\).

We proceed by induction on~\(m\). The claim is true for \(m=1\) by the above. For \(m>1\), we have
\[\bigsect_{j=1}^m\ker y_j=\bigl(\bigsect_{j=1}^{m-1}\ker y_j\bigr)\sect\ker y_m\]
Therefore, by the inclusion-exclusion principle for dimension (Exercise~12.7) and the induction hypothesis, we have
\begin{align*}
\dim\bigsect_{j=1}^m\ker y_j&=\dim\Bigl[\bigl(\bigsect_{j=1}^{m-1}\ker y_j\bigr)\sect\ker y_m\Bigr]\\
	&=\dim\bigsect_{j=1}^{m-1}\ker y_j+\dim\ker y_m-\dim\Bigl[\bigsect_{j=1}^{m-1}\ker y_j+\ker y_m\Bigr]\\
	&\ge[n-(m-1)]+(n-1)-n\\
	&=n-m\qedhere
\end{align*}
\end{proof}
\begin{rmk}
This result implies that a homogeneous system of \(m\)~linear equations in \(n\)~variables always has a nontrivial solution when \(m<n\). Indeed, consider the system
\begin{equation*}
\left\lbrace
\begin{aligned}
\alpha_{11}x_1+\cdots+\alpha_{1n}x_n&=0\\
\phantom{\alpha_{i1}}\vdots\phantom{+\cdots+a_{in}}\vdots\phantom{x_{in}}&\phantom{=}\vdots\\
\alpha_{m1}x_1+\cdots+\alpha_{mn}x_n&=0
\end{aligned}
\right.\qquad(\alpha_{ij}\in\F)
\end{equation*}
For \(\vec{x}=(x_1,\ldots,x_n)\in\F^n\), define~\(y_j\) by \([x,y_j]=\sum_i\alpha_{ij}x_i\) for \(1\le j\le m\). Clearly this system has a nontrivial solution if and only if there exists a nonzero \(\vec{x}\in\F^n\) such that \([x,y_j]=0\) for all \(1\le j\le m\), which is true by this result.
\end{rmk}

\begin{exer}[7]
If \(V\)~is an \(n\)-dimensional vector space and \(0\le m\le n\), then the number of \(m\)-dimensional subspaces of~\(V\) is equal to the number of \((n-m)\)-dimensional subspaces.
\end{exer}
\begin{proof}
Fix a basis of~\(V\) and assume that \(V=V'=V''\) (Theorems 15.2~and~16.1). Now the mapping \(M\mapsto M^0\) sends each \(m\)-dimensional subspace to an \((n-m)\)-dimensional subspace (Theorem~1). Moreover, this mapping is its own inverse (Theorem~2), hence it is bijective and witnesses cardinal equality.
\end{proof}

% Section 20 
\subsection*{\S~20}
\begin{exer}[3]
There exists a vector space~\(V\) with subspaces \(M,N_1,N_2\) such that \(V=M\dsum N_1=M\dsum N_2\) but \(N_1\ne N_2\).
\end{exer}
\begin{proof}
Let \(V=\R^2\), \(M\)~be the subspace consisting of vectors of the form~\((x,0)\) (the horizontal axis), \(N_1\)~be the subspace consisting of vectors of the form~\((0,y)\) (the vertical axis), and \(N_2\)~be the subspace consisting of vectors of the form~\((x,x)\) (the diagonal line \(y=x\)).
\end{proof}
\begin{rmk}
This result shows that there is no cancellation law for direct sums.
\end{rmk}

\begin{exer}[4]
Let \(U,V,W\) be vector spaces.
\begin{enumerate}
\item[(a)] \((U\dsum V)\dsum W\iso U\dsum(V\dsum W)\)
\item[(b)] \(U\dsum V\iso V\dsum U\)
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] The mapping \(\pair{\pair{u}{v}}{w}\mapsto\pair{u}{\pair{v}{w}}\) is clearly bijective and linear.
\item[(b)] The mapping \(\pair{u}{v}\mapsto\pair{v}{u}\) is clearly bijective and linear.\qedhere
\end{enumerate}
\end{proof}

% Section 22
\subsection*{\S~22}
\begin{exer}[4]
Let \(V\)~be a vector space and \(M\)~be a subspace of~\(V\). In addition, let \(\pi:V\to V/M\) be the mapping \(x\mapsto x+M\).
\begin{enumerate}
\item[(a)] The mapping \(\phi:y\mapsto y\pi\) is an isomorphism from~\((V/M)'\) to~\(M^0\).
\item[(b)] The mapping \(\psi:y+M^0\mapsto\restrict{y}{M}\) is an isomorphism from~\(V'/M^0\) to~\(M'\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] The mapping~\(\phi\) is defined from~\((V/M)'\) into~\(V'\). It is injective since if \(y\pi=z\pi\), then
\[y(x+M)=y(\pi(x))=z(\pi(x))=z(x+M)\]
for all \(x\in V\), so \(y=z\). To see that \(\ran\phi\subseteq M^0\), note that if \(y\pi\in\ran\phi\), then \(y\pi(x)=y(M)=0\) for all \(x\in M\), so \(y\pi\in M^0\). Conversely, if \(z\in M^0\), define~\(y\) on~\(V/M\) by \(x+M\mapsto z(x)\). Note \(y\)~is well defined and linear since \(z\in M^0\), so \(y\in(V/M)'\), and \(\phi(y)=y\pi=z\). Therefore \(\ran\phi=M^0\). Finally, \(\phi\)~is linear, hence \(\phi:(V/M)'\iso M^0\).
\item[(b)] The mapping~\(\psi\) is clearly well defined and injective from~\(V'/M^0\) to~\(M'\) since
\begin{align*}
y+M^0=z+M^0&\iff y-z\in M^0\\
	&\iff (y-z)(x)=0\quad(x\in M)\\
	&\iff y(x)=z(x)\quad(x\in M)\\
	&\iff\restrict{y}{M}=\restrict{z}{M}
\end{align*}
It is also surjective since if \(z\in M'\), then there exists \(y\in V'\) with \(\restrict{y}{M}=z\). Indeed, if \(N\)~is any complement of~\(M\) in~\(V\), define \(y(u+v)=z(u)\) for \(u\in M\) and \(v\in N\). Finally, it is clearly linear, hence an isomorphism.\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
Just remember \((V/M)'\iso V'/M'\) (Theorems 20.1~and~22.1).
\end{rmk}

\begin{exer}[5]
If \(V\)~is finite-dimensional and \(W=V\dsum V'\), then the mapping \(\pair{x}{y}\mapsto\pair{y}{x}\) is an isomorphism from~\(W\) to~\(W'\).
\end{exer}
\begin{proof}
By taking the dual of the direct sum~\(W\) (Theorem~20.1) and applying reflexivity of~\(V\) (Theorem~16.1), we obtain
\[W'=(V\dsum V')'=V'\dsum V''=V'\dsum V\]
Hence the mapping is indeed a mapping from~\(W\) to~\(W'\). It is clearly bijective and linear, hence an isomorphism.
\end{proof}

% Section 23
\subsection*{\S~23}
\begin{exer}[1]
Let \(V=\R^n\dsum\R^n\).
\begin{enumerate}
\item[(a)] If \(w\)~is a bilinear form on~\(V\), then there exist unique scalars \(\alpha_{ij}\in\R\) for \(1\le i,j\le n\) such that
\[w(\vec{x},\vec{y})=\sum_{i=1}^n\sum_{j=1}^n\alpha_{ij}x_iy_j\]
for all \(\vec{x}=(x_1,\ldots,x_n)\) and \(\vec{y}=(y_1,\ldots,y_n)\) in~\(\R^n\).
\item[(b)] If \(z\)~is a linear functional on the space of all bilinear forms on~\(V\), then there exist unique scalars \(\beta_{ij}\in\R\) for \(1\le i,j\le n\) such that
\[z(w)=\sum_{i=1}^n\sum_{j=1}^n\alpha_{ij}\beta_{ij}\]
for all~\(w\) with \(\alpha_{ij}\)~as in~(a).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] Let \(\{\vec{e_1},\ldots,\vec{e_n}\}\) be the standard basis of~\(\R^n\) (where \(\vec{e_i}=(\delta_{i1},\ldots,\delta_{in})\)) and let \(\alpha_{ij}=w(\vec{e_i},\vec{e_j})\) for \(1\le i,j\le n\). Then for \(\vec{x}=(x_1,\ldots,x_n)\) and \(\vec{y}=(y_1,\ldots,y_n)\), \(\vec{x}=\sum_i x_i\vec{e_i}\) and \(\vec{y}=\sum_j y_j\vec{e_j}\), so
\[w(\vec{x},\vec{y})=\sum_i\sum_jx_iy_jw(\vec{e_i},\vec{e_j})=\sum_i\sum_j\alpha_{ij}x_iy_j\]
by bilinearity of~\(w\) (see the proof of Theorem~1). If scalars \(\alpha_{ij}'\in\R\) also satisfy this condition, then
\[\alpha_{ij}=w(\vec{e_i},\vec{e_j})=\alpha_{ij}'\qquad(1\le i,j\le n)\]
\item[(b)]
Again, let \(\{\vec{e_1},\ldots,\vec{e_n}\}\) be the standard basis of~\(\R^n\) and let \(w_{pq}\) (\(1\le p,q\le n\)) be the corresponding `standard' basis of the space of all bilinear forms on~\(V\) (Theorem~2). Let \(\beta_{ij}=z(w_{ij})\) for \(1\le i,j\le n\). Then for a bilinear form~\(w\) as in~(a), \(w=\sum_i\sum_j\alpha_{ij}w_{ij}\) by bilinearity of~\(w\) (see the proof of Theorem~2), so
\[z(w)=\sum_i\sum_j\alpha_{ij}z(w_{ij})=\sum_i\sum_j\alpha_{ij}\beta_{ij}\]
by linearity of~\(z\) (see the proof of Theorem~15.1). If scalars \(\beta_{ij}'\in\R\) also satisfy this condition, then
\[\beta_{ij}=z(w_{ij})=\beta_{ij}'\qquad(1\le i,j\le n)\qedhere\]
\end{enumerate}
\end{proof}
\begin{rmk}
If \(\vec{x}=(x_1,\ldots,x_n)\) and \(\vec{y}=(y_1,\ldots,y_n)\), let \(\beta_{ij}=x_iy_j\) for \(1\le i,j\le n\). Then \(z\)~in~(b) is the tensor product of \(\vec{x}\)~and~\(\vec{y}\) (Definition~25.1), which is intuitively just~\(\beta\).
\end{rmk}

\begin{exer}[2]
Let \(V=\C^2\dsum\C^2\).
\begin{enumerate}
\item[(a)] The mapping \(\pair{(x_1,x_2)}{(y_1,y_2)}\mapsto x_1y_1\) is a degenerate bilinear form on~\(V\).
\item[(b)] The mapping \(\pair{(x_1,x_2)}{(y_1,y_2)}\mapsto x_1y_1+x_2y_2\) is a non-degenerate bilinear form on~\(V\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] The mapping is clearly bilinear. It is degenerate since \(\pair{(0,1)}{(y_1,y_2)}\mapsto 0\) for all \((y_1,y_2)\in\C^2\).
\item[(b)] The mapping is clearly bilinear. If \(x_1,x_2\in\C\) and \(x_1y_1+x_2y_2=0\) for all \(y_1,y_2\in\C\), then in particular \(x_1=x_1\cdot1+x_2\cdot0=0\) and \(x_2=x_1\cdot0+x_2\cdot1=0\). Similarly if \(y_1,y_2\in\C\) and \(x_1y_1+x_2y_2=0\) for all \(x_1,x_2\in\C\), then \(y_1=y_2=0\). Therefore the mapping is non-degenerate.\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
The mapping in~(b), restricted to~\(\R^n\), is just the dot product, which reflects the extent to which two nonzero vectors point in the same direction geometrically.
\end{rmk}

\begin{exer}[5]
The mapping \(w:\pair{(x_1,x_2)}{(y_1,y_2)}\mapsto x_1y_2-x_2y_1\) is a nonzero bilinear form on~\(\C^2\dsum\C^2\) with \(w(x,x)=0\) for all \(x\in\C^2\).
\end{exer}
\begin{proof}
The mapping~\(w\) is clearly bilinear and nonzero, and
\[w((x_1,x_2),(x_1,x_2))=x_1x_2-x_1x_2=0\qquad((x_1,x_2)\in\C^2)\qedhere\]
\end{proof}
\begin{rmk}
This mapping is just the 2-by-2 determinant, which reflects the linear dependence (collinearity) of two vectors geometrically. See Exercise~7.5.
\end{rmk}

% Section 25
\subsection*{\S~25}
\begin{exer}[2]
Let \(\P_{n,m}\)~be the space of all polynomials~\(z(s,t)\) such that either \(z=0\) or else \(\deg_s z\le n-1\) and \(\deg_t z\le m-1\). Then there exists an isomorphism \(\P_n\tprod\P_m\iso\P_{n,m}\) such that \(x\tprod y\mapsto xy\) for all \(x\in\P_n\) and \(y\in\P_m\).
\end{exer}
\begin{proof}
Let \(\{1,s,\ldots,s^{n-1}\}\) be a basis of~\(\P_n\) and \(\{1,t,\ldots,t^{m-1}\}\) be a basis of~\(\P_m\). Then \(\{s^i\tprod t^j\}\)~is a basis of~\(\P_n\tprod\P_m\) (Theorem~1). We claim that \(\{s^it^j\}\)~is a basis of~\(\P_{n,m}\). Indeed, the set is linearly independent, since if
\[z(s,t)=\sum_i\sum_j\alpha_{ij}s^it^j=0\]
then \(z\)~has infinitely many roots, so \(\alpha_{ij}=0\) for all~\(i,j\). Also, the set spans~\(\P_{n,m}\) by definition.

Let \(\pi\)~be the isomorphism from~\(\P_n\tprod\P_m\) to~\(\P_{n,m}\) such that \(s^i\tprod t^j\mapsto s^it^j\) for all~\(i,j\) (Theorem~9.1). Then for \(x(s)=\sum_i\alpha_is^i\in\P_n\) and \(y(t)=\sum_j\beta_jt^j\in\P_m\),
\begin{align*}
\pi(x\tprod y)&=\pi\bigl[\,\sum_i\sum_j\alpha_i\beta_j(s^i\tprod t^j)\bigr]\\
	&=\sum_i\sum_j\alpha_i\beta_j\pi(s^i\tprod t^j)\\
	&=\sum_i\sum_j\alpha_i\beta_js^i t^j\\
	&=\bigl(\,\sum_i\alpha_is^i\bigr)\bigl(\,\sum_j\beta_jt^j\bigr)\\
	&=xy\qedhere
\end{align*}
\end{proof}

\begin{exer}[3]
Let \(U,V,W\) be finite-dimensional vector spaces.
\begin{enumerate}
\item[(a)] \(U\tprod V\iso V\tprod U\)
\item[(b)] \((U\tprod V)\tprod W\iso U\tprod(V\tprod W)\)
\item[(c)] \(U\tprod(V\dsum W)=(U\tprod V)\dsum(U\tprod W)\)
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(a)] The map \(x\tprod y\mapsto y\tprod x\) is an isomorphism.
\item[(b)] The map \((x\tprod y)\tprod z\mapsto x\tprod(y\tprod z)\) is an isomorphism.
\item[(c)] The two spaces have the same basis.\qedhere
\end{enumerate}
\end{proof}

\begin{exer}[4]
There exists a finite-dimensional vector space~\(V\) and vectors \(x,y\in V\) such that \(x\tprod y\ne y\tprod x\).
\end{exer}
\begin{proof}
Let \(V=\C^2\), so \(\vec{e_1}=(1,0)\) and \(\vec{e_2}=(0,1)\). Then
\[\vec{e_1}\tprod\vec{e_2}=
	\begin{pmatrix}
		0&1\\
		0&0
	\end{pmatrix}\neq
	\begin{pmatrix}
		0&0\\
		1&0
	\end{pmatrix}
=\vec{e_2}\tprod\vec{e_1}\qedhere\]
\end{proof}
\begin{rmk}
This result shows that the vector tensor product is not commutative.
\end{rmk}

% Section 31
\subsection*{\S~31}
\begin{exer}[3]
The mapping
\[w:\pair{(x_1,x_2,x_3)}{(y_1,y_2,y_3)}\mapsto x_1y_2-x_2y_1\]
is a nonzero alternating bilinear form on~\(\C^3\). The vectors \(\vec{x}=(1,0,0)\) and \(\vec{y}=(1,0,1)\) are linearly independent but \(w(\vec{x},\vec{y})=0\).
\end{exer}
\begin{proof}
The claims are clearly true (see also Exercise~23.5).
\end{proof}
\begin{rmk}
The mapping is just the 2-by-2 determinant of the projections of the vectors into the x-y plane. The result just reflects the fact that two vectors in x-y-z space can fail to be collinear while their projections into the x-y plane are collinear.
\end{rmk}

% Chapter II
\section*{Chapter~II}
% Section 49
\subsection*{\S~49}
\begin{exer}[4]
Let \(V\)~be a vector space and \(E\)~and~\(F\) be projections on~\(V\).
\begin{enumerate}
\item[(a)] \(\ran(E)=\ran(F)\) if and only if \(EF=F\) and \(FE=E\).
\item[(b)] \(\ker(E)=\ker(F)\) if and only if \(EF=E\) and \(FE=F\).
\end{enumerate}
\end{exer}
\begin{proof}
Recall for a projection~\(P\) on~\(V\), \(V=\ran(P)\dsum\ker(P)\) and (Theorem~41.2)
\[\ran(P)=\{\,x\in V\mid Px=x\,\}\qquad\ker(P)=\{\,x\in V\mid Px=0\,\}\]
\begin{enumerate}
\item[(a)]
\begin{fwd}
If \(x\in V\), then \(Ex\in\ran(E)\subseteq\ran(F)\), so \(FEx=F(Ex)=Ex\). Therefore \(FE=E\). Similarly \(EF=F\).
\end{fwd}
\begin{bwd}
If \(x\in\ran(E)\), then \(x=Eu\) for some \(u\in V\), so
\[Fx=F(Eu)=FEu=Eu=x\]
and hence \(x\in\ran(F)\). Therefore \(\ran(E)\subseteq\ran(F)\). Similarly \(\ran(F)\subseteq\ran(E)\) and hence \(\ran(E)=\ran(F)\).
\end{bwd}
\item[(b)]
\begin{fwd}
Since \(V=\ran(E)\dsum\ker(E)\), if \(x\in V\) there exist \(u\in\ran(E)\) and \(v\in\ker(E)\) with \(x=u+v\). Now
\begin{align*}
FEx&=FE(u+v)&&\\
	&=FEu+FEv&&\\
	&=Fu+F0&&\text{since \(u\in\ran(E)\) and \(v\in\ker(E)\)}\\
	&=Fu+Fv&&\text{since \(\ker(E)\subseteq\ker(F)\)}\\
	&=F(u+v)\\
	&=Fx
\end{align*}
Therefore \(FE=F\). Similarly \(EF=E\).
\end{fwd}
\begin{bwd}
If \(x\in\ker(E)\), then
\[Fx=FEx=F(Ex)=F0=0\]
so \(x\in\ker(F)\). Therefore \(\ker(E)\subseteq\ker(F)\). Similarly \(\ker(F)\subseteq\ker(E)\) and hence \(\ker(E)=\ker(F)\).\qedhere
\end{bwd}
\end{enumerate}
\end{proof}
\begin{rmk}
By (a)~and~(b), \(E=F\) if and only if \(\ran(E)=\ran(F)\) and \(\ker(E)=\ker(F)\). In other words, projections are characterized by their ranges and null spaces.
\end{rmk}

\begin{exer}[5]
If \(E_1,\ldots,E_k\) are projections on~\(V\) with the same range and \(\alpha_1,\ldots,\alpha_k\) are scalars such that \(\sum_i\alpha_i=1\), then \(E=\sum_i\alpha_i E_i\) is a projection.
\end{exer}
\begin{proof}
By Exercise~4(a), we have
\begin{align*}
E^2&=\bigl(\,\sum_i\alpha_iE_i\bigr)^2&&\\
	&=\sum_i\sum_j\alpha_i\alpha_j E_iE_j&&\\
	&=\sum_i\sum_j\alpha_i\alpha_j E_j&&\text{since \(\ran(E_i)=\ran(E_j)\)}\\
	&=\bigl(\,\sum_i\alpha_i\bigr)\bigl(\,\sum_j\alpha_jE_j\bigr)&&\\
	&=1\cdot E&&\text{since \(\textstyle\sum_i\alpha_i=1\)}\\
	&=E
\end{align*}
Therefore \(E\)~is idempotent, and hence a projection (Theorem~41.1).
\end{proof}

% Section 55
\subsection*{\S~55}
\begin{rmk}
We clarify Halmos' proof (p.~105) that the algebraic multiplicity of an eigenvalue is at least as great as its geometric multiplicity. As in the proof, let \(A\)~be a linear transformation, \(\lambda_0\)~be an eigenvalue, \(M\)~be the corresponding eigenspace, and \(A_0=\restrict{A}{M}\). If \(\lambda\)~is arbitrary, observe that \(M\)~is also invariant under \(A-\lambda\), and \(\restrict{(A-\lambda)}{M}=\restrict{A}{M}-\lambda=A_0-\lambda\). Therefore by the determinant of quotient maps (\S~53, p.~100),
\[\det(A-\lambda)=\det(A_0-\lambda)\cdot\det((A-\lambda)/M)\]
So \(\det(A_0-\lambda)\) is a factor of \(\det(A-\lambda)\). Now \((A_0-\lambda)x=(\lambda_0-\lambda)x\) for all \(x\in M\), so \(\det(A_0-\lambda)=(\lambda_0-\lambda)^m\), where \(m=\dim M\), by the determinant of scalar maps (\S~53, p.~99). It follows that the algebraic multiplicity of~\(\lambda_0\) as a root of~\(\det(A-\lambda)\) is at least~\(m\), which is just the geometric multiplicity of~\(\lambda_0\).
\end{rmk}

% Section 56
\subsection*{\S~56}
\begin{rmk}
We clarify Halmos' remark (p.~107) that \(\det(A-\alpha_{ii})=0\) for each of the diagonal entries~\(\alpha_{ii}\) in an upper triangular matrix~\([A]\) for~\(A\). Observe from the matrix~\([A-\alpha_{ii}]\) (on the same basis) that \(\restrict{(A-\alpha_{ii})}{M_i}\) maps the \(i\)-dimensional subspace~\(M_i\) into the \((i-1)\)-dimensional subspace~\(M_{i-1}\). Hence by rank nullity (Theorem~50.1), \(\dim\ker(A-\alpha_{ii})\ne0\), so \(A-\alpha_{ii}\)~is not invertible (\S~49, p.~89), so \(\det(A-\alpha_{ii})=0\) (\S~53, p.~99).

Alternately, observe from the equation for the determinant (Equation~53.2) that the determinant of an upper triangular matrix is the product of its diagonal entries. Since \([A-\lambda]\) (on the same basis) is upper triangular,
\[\det(A-\lambda)=\prod_i(\alpha_{ii}-\lambda)\]
This shows \(\det(A-\alpha_{ii})=0\), so \(\alpha_{ii}\)~is an eigenvalue of~\(A\), and \(\alpha_{ii}\)~appears on the diagonal of~\([A]\) as many times as its algebraic multiplicity.
\end{rmk}

\begin{rmk}
We show that if \(A\)~is a linear transformation and \(p\)~is a polynomial, then the eigenvalues of~\(p(A)\), including algebraic multiplicities, are precisely the values~\(p(\lambda)\) where \(\lambda\)~ranges over the eigenvalues of~\(A\) (p.~108). Indeed, fix a basis on which \([A]=(\alpha_{ij})\)~is upper triangular. Then \([p(A)]=p([A])\) on the same basis is also upper triangular with diagonal entries~\(p(\alpha_{ii})\). The result now follows from the previous remark.
\end{rmk}

% References
\begin{thebibliography}{0}
\bibitem{halmos87} Halmos, P. \textit{Finite Dimensional Vector Spaces.} Springer, 1987.
\end{thebibliography}
\end{document}